{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 SVM",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ1Dt9PSStutJxilP7v51i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_34GkET0ZWm0"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.impute import SimpleImputer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC, NuSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "'''\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "'''\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD4Cm0zuZe4G"
      },
      "source": [
        "tweet=pd.read_excel('file path', usecols=['col1', 'col2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNnxuqA9JXD"
      },
      "source": [
        "tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgUleL1x9J5U"
      },
      "source": [
        "#import seaborn as sns\n",
        "#sns.countplot(x='sentiment', data=tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3yAHDcB9NsE"
      },
      "source": [
        "tweet.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG9spjn09On2"
      },
      "source": [
        "X = tweet.tweet\n",
        "y = tweet.sentiment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDUEGZxa9Q5N"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cmxfbs59TBG"
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nl78kWv9W5p"
      },
      "source": [
        "y_test.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHb2ucOf9dQy"
      },
      "source": [
        "# Removing stop words\n",
        "def get_stop_words(path):\n",
        "    #\"stop_words.txt\"\n",
        "    stop_words = []\n",
        "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
        "        stop_words = myfile.readlines()\n",
        "    stop_words = [word.strip() for word in stop_words]\n",
        "    return stop_words\n",
        "stop_words = get_stop_words('/content/stop_words_arabic.txt')\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('vect', CountVectorizer(stop_words=stop_words)), ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "663eGa7a9gJU"
      },
      "source": [
        "# Extracting features from text files\n",
        "count_vect = CountVectorizer(stop_words=stop_words)\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "print('X_train_counts.shape', X_train_counts.shape)\n",
        "    # TF-IDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "print ('X_train_tfidf.shape',X_train_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBGn48Mh9g3c"
      },
      "source": [
        "# Training Support Vector Machines - SVM and calculating its performance\n",
        "\n",
        "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
        "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42))])\n",
        "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
        "predicted_svm = text_clf_svm.predict(X_test)\n",
        "print(np.mean(predicted_svm == y_test))\n",
        "\n",
        "predictions = text_clf_svm.predict(X_test)\n",
        "\n",
        "        # macro accuracy (macro average)\n",
        "\n",
        "        # precision and recall\n",
        "\n",
        "print(\"Precision Score : \",precision_score(y_test, predictions, \n",
        "                                           average='micro'))\n",
        "print(\"Recall Score : \",recall_score(y_test, predictions, \n",
        "                                           average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxc1CQEC9k6f"
      },
      "source": [
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4XVo0Q-9nBV"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Oranges)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = numpy.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbVY26rk9pqx"
      },
      "source": [
        "class_labels = ['Positive', 'Negative', 'Neutral']\n",
        "cm = confusion_matrix(y_test,predictions).T\n",
        "plot_confusion_matrix(cm, classes = class_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}